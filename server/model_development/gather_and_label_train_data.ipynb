{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gather_and_label_train_data","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN6E7DfMw4bH3Mt2z5ToqFw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zY8mHUvwW-UV"},"source":["# Gather and Label Train Data\n","\n","This sheet is used to load, preprocess, and label data.\n","\n","This sheet should be used in Google Collab"]},{"cell_type":"markdown","metadata":{"id":"NjxVOUa4XIMt"},"source":["# Install and Import Packages"]},{"cell_type":"code","metadata":{"id":"51A8Ym0F2obB"},"source":["from google.colab import auth\n","auth.authenticate_user()\n","import gspread\n","from oauth2client.client import GoogleCredentials\n","gc = gspread.authorize(GoogleCredentials.get_application_default())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8YDhtNz3BF4"},"source":["pip install --upgrade \"ibm-watson>=5.1.0\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4iNcFgh26rN"},"source":["pip install jsbeautifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUIkTfrrDDzt"},"source":["!pip install contractions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tLMrjAHZErTv"},"source":["!pip install emoji"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6AXWq76KRgY"},"source":["google_drive_path = \"/content/drive/My Drive/catchfire/\"\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DdUnbv_22dD"},"source":["# Data Science Packages\n","import pandas as pd\n","import sklearn.linear_model\n","import tensorflow as tf\n","import numpy as np\n","import json\n","import string\n","from collections import Counter\n","\n","# JSON\n","import jsbeautifier\n","opts = jsbeautifier.default_options()\n","opts.indent_size = 2\n","\n","# IBM Watson\n","from ibm_watson import ToneAnalyzerV3\n","from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n","\n","# API Calls\n","import requests\n","from requests.auth import HTTPBasicAuth\n","\n","# NLP Packages\n","import re\n","import spacy\n","import nltk\n","nltk.download('stopwords')\n","from nltk.tokenize.toktok import ToktokTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from bs4 import BeautifulSoup\n","from nltk.util import ngrams\n","import contractions \n","from emoji import UNICODE_EMOJI\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdpnqqrRXS0N"},"source":["# Connect to Google Sheet and Download Data"]},{"cell_type":"code","metadata":{"id":"eq6MOijE29nH"},"source":["wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1vmCtCwZqPtFIEewhM1CWT7C1QuwJ3gB8bbK8S9Qtwlg/edit?fbclid=IwAR2cIq9qDzU2Q0KyH_hiAmC7Ibm2P3o0rvO60nJau1R7rgL0O_E9UsGThP4#gid=1276133210')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3D1oJ0b3GwH"},"source":["tickers = [\"JPM\",\n","           \"AAPL\",\n","           \"GOOG\",\n","           \"MSFT\",\n","           \"WMT\",\n","           \"AMZN\",\n","           \"DIS\",\n","           \"PFE\",\n","           \"XOM\",\n","           \"CVS\",\n","           \"PZZA\",\n","           \"TREE\",\n","           \"TMHC\",\n","           \"LOPE\",\n","           \"DISH\",\n","           \"CROX\",\n","           \"MMP\",\n","           \"THS\",\n","           \"ARMK\",\n","           \"BBY\",\n","           \"JLL\",\n","           \"BTC\",\n","           \"ETH\",\n","           \"XRP\",\n","           \"VET\",\n","           \"USDT\",\n","           \"ALGO\",\n","           \"XMR\",\n","           \"ATOM\",\n","           \"RVN\",\n","           \"AAVE\",\n","           \"BNB\",\n","           \"SPY\",\n","           \"UCXY\",\n","           \"SQQQ\",\n","           \"XLF\",\n","           \"GLD\",\n","           \"EEM\",\n","           \"XLE\",\n","           \"IWM\",\n","           \"QQQ\",\n","           \"SLV\"]\n","\n","data_frames = []\n","           \n","for i in range(0,len(tickers)):\n","  sheet = wb.worksheet(tickers[i])\n","  df = pd.DataFrame(sheet.get_all_values())\n","  new_header = df.iloc[0] #grab the first row for the header\n","  df = df[1:] #take the data less the header row\n","  df.columns = new_header #set the header row as the df header\n","  data_frames.append(df)\n","\n","unlabeled_data = pd.concat(data_frames).reset_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3cowB_-6JZ8"},"source":["unlabeled_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ak7SYvwy6Khi"},"source":["feature_cols = [\n","  \"Content\"\n","]\n","unlabeled_data_feature = unlabeled_data[feature_cols]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6JJYvpERXXmR"},"source":["# Preprocess Text for Labeling\n","\n","This will be the same preprocessing for inference also."]},{"cell_type":"code","metadata":{"id":"pAA2LOZi6Smj"},"source":["def lower_case(text):\n","    return text.lower()\n","\n","def strip_links(text):\n","    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n","    links         = re.findall(link_regex, text)\n","    for link in links:\n","        text = text.replace(link[0], ', ')    \n","    return text\n","\n","def space_comma(text):\n","    pattern = ',' \n","    text = re.sub(pattern, ' , ', text)\n","    return text\n","\n","def space_emoji(text):\n","    try:\n","      # Wide UCS-4 build\n","      oRes = re.compile(u'(['\n","          u'\\U0001F300-\\U0001F64F'\n","          u'\\U0001F680-\\U0001F6FF'\n","          u'\\u2600-\\u26FF\\u2700-\\u27BF]+)', \n","          re.UNICODE)\n","    except re.error:\n","      # Narrow UCS-2 build\n","      oRes = re.compile(u'(('\n","          u'\\ud83c[\\udf00-\\udfff]|'\n","          u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n","          u'[\\u2600-\\u26FF\\u2700-\\u27BF])+)', \n","          re.UNICODE)\n","    text = oRes.sub(r'  \\1  ', text) \n","    return text  \n","\n","def space_period(text):\n","    pattern = '\\.' \n","\n","    if \".\" in text:\n","      last_position = text.rindex(\".\")\n","      text = text[:last_position] + '' + text[last_position + 1:]\n","\n","    text = re.sub(pattern, ' ; ', text)\n","    return text\n","\n","def consolidate_punc(text):\n","    pattern = r'[.!?,;\\\\]'\n","\n","    if \"!\" in text:\n","      excl = True;\n","    else:\n","      excl = False;\n","\n","    if \"?\" in text:\n","      question = True;\n","    else:\n","      question = False;\n","\n","    text = re.sub(pattern, '', text)\n","\n","    if (excl):\n","      text = text + \" ! \";\n","    elif (question):\n","      text = text + \" ? \";\n","    else:\n","      text = text + \" . \";\n","\n","    return text\n","\n","def remove_chars(text):\n","    text = text.replace(\"[\",\"\")\n","    text = text.replace(\"]\",\" , \")\n","    text = text.replace(\"(\",\"\")\n","    text = text.replace(\")\",\" , \")\n","    text = text.replace(\"&amp\",\"\")\n","    text = text.replace(\"&gt\",\"\")\n","    text = text.replace(\"&lt\",\"\")\n","    text = text.replace(\"*\",\"\")\n","    text = text.replace(\"-\",\" , \")\n","    text = text.replace(\"|\",\" \")\n","    text = text.replace(\":\",\" \")\n","    text = text.replace(\"@\",\" \")\n","    text = text.replace(\"#\",\" \")\n","    text = text.replace(\"$\",\" \")\n","    text = text.replace(\"/\",\" \")\n","    text = text.replace(\"\\\\\",\" \")\n","    return text\n","\n","def remove_numbers(text):\n","    pattern = r'\\d+'\n","    text = re.sub(pattern, '', text)\n","    return text\n","\n","def expand_contractions(text):\n","    text = contractions.fix(text)\n","    return text\n","\n","tokenizer=ToktokTokenizer()\n","\n","def tokenize(text):\n","    return tokenizer.tokenize(text)\n","\n","def join_text(text):\n","    return \" \".join(text)\n","\n","def remove_dup(text):\n","    text = text.replace(\", ,\",\" ,\")\n","    text = text.replace(\"; ;\",\" ;\")\n","    return text\n","\n","# Make everything lowercase.\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(lower_case)\n","# Take out all links.\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(strip_links)\n","# Seperate commas out into seperate token. ',' -> ' , '\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(space_comma)\n","# Seperate periods out into seperate token and modify. '.' -> ' ; '\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(space_period)\n","# Seperate emojis out into seperate token and modify. \n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(space_emoji)\n","# Look for special punctuation and add punctuation to end of sentence\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(consolidate_punc)\n","# Remove certian characters.\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(remove_chars)\n","# Remove numbers.\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(remove_numbers)\n","# Expand Contractions\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(expand_contractions)\n","# Tokenize and Join Text. (Make uniform spaces)\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(tokenize)  \n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(join_text)  \n","# Remove some duplication that could occur from preprocessing\n","unlabeled_data_feature['Content'] = unlabeled_data_feature['Content'].map(remove_dup)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sEq1EEdiYJsk"},"source":["# Vectorize Words to Explore Vocab. \n"]},{"cell_type":"code","metadata":{"id":"rhpY5umNGSzQ"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","cvectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, ngram_range=(1, 2), max_features = 5000) \n","\n","bow_data_features = cvectorizer.fit_transform(unlabeled_data_feature['Content'])\n","bow_data_features = bow_data_features.toarray()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZrOWD0wGmaL"},"source":["vocab = cvectorizer.get_feature_names()\n","vocab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-Fn7N3_IwrK"},"source":["data_item = 145555\n","\n","print(unlabeled_data['Content'][data_item])\n","print(unlabeled_data_feature['Content'][data_item])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7BsHwGLLbFu"},"source":["len(unlabeled_data_feature['Content'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p7OZ-XL5LbBg"},"source":["for i in range(130000,130100):\n","  print(unlabeled_data['Content'][i])\n","  print(unlabeled_data_feature['Content'][i])\n","  print(\"---------------------------------\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44Uk-1bLYROt"},"source":["# Label Data"]},{"cell_type":"code","metadata":{"id":"LZauEgI5Jsne"},"source":["authenticator = IAMAuthenticator('-')\n","tone_analyzer = ToneAnalyzerV3(\n","    version='2021-03-26',\n","    authenticator=authenticator\n",")\n","\n","tone_analyzer.set_service_url(\"https://api.us-south.tone-analyzer.watson.cloud.ibm.com/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CC_-dwBxNJGg"},"source":["google_drive_path = \"/content/drive/My Drive/catchfire/training_labeled_data_5_12/\"\n","len(unlabeled_data_feature['Content'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CtcEN-q0YZ_2"},"source":["Go through and save labeled data in blocks. We need to process limited chunks of sentences at a time."]},{"cell_type":"code","metadata":{"id":"gZUWb7SyJ3Nm"},"source":["for block in range(1650,1651):\n","  text = \"\"\n","  content = unlabeled_data_feature['Content']\n","\n","  for i in range(block*100,(block+1)*100):\n","    text = text + unlabeled_data_feature['Content'][i] + \" \"\n","\n","  tone_analysis = tone_analyzer.tone(\n","    {'text': text},\n","    content_type='application/json',\n","    sentences = True\n","  ).get_result()\n","\n","  filename = \"label_5_11_\"+ str(block) +\".json\"\n","  \n","  print(block)\n","\n","  with open(google_drive_path+filename,\"w\") as json_file:\n","   json.dump(tone_analysis, json_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WBGTqNvjpJi"},"source":["  filename = \"label_5_11_\"+ str(block) +\".json\"\n","\n","  with open(google_drive_path+filename) as f:\n","    data = json.load(f)\n","\n","  data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bq5ivW8-YiKr"},"source":["Load data back from saved labeled data."]},{"cell_type":"code","metadata":{"id":"EgK7nnQnOGD3"},"source":["label_data = pd.DataFrame(index=np.arange(1), columns=np.arange(8))\n","label_data.columns = ['text','anger','fear','joy','sadness','analytical','confident','tentative']\n","\n","for block in range(0,1650):\n","  print(block)\n","  filename = \"label_5_11_\"+ str(block) +\".json\"\n","\n","  with open(google_drive_path+filename) as f:\n","    data = json.load(f)\n","\n","  for sent in data['sentences_tone']:\n","    to_append = {\"text\" : \"\",\n","                \"anger\" :0,\n","                \"fear\":0,\n","                \"joy\":0,\n","                \"sadness\":0,\n","                \"analytical\":0,\n","                \"confident\":0,\n","                \"tentative\":0} \n","\n","    to_append['text'] = sent['text'];\n","    for tone in sent['tones']:\n","        to_append[tone['tone_id']] = tone['score']\n","\n","    label_data.loc[i] = to_append\n","    i = i+1\n","\n","label_data['anger'] = label_data['anger'].astype(float)\n","label_data['fear'] = label_data['fear'].astype(float)\n","label_data['joy'] = label_data['joy'].astype(float)\n","label_data['sadness'] = label_data['sadness'].astype(float)\n","label_data['analytical'] = label_data['analytical'].astype(float)\n","label_data['confident'] = label_data['confident'].astype(float)\n","label_data['tentative'] = label_data['tentative'].astype(float)\n","\n","label_data.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VEqSURyZYmSu"},"source":["Save to Excel File"]},{"cell_type":"code","metadata":{"id":"Ew0w78_MPLs4"},"source":["label_data.to_excel(google_drive_path+\"labeled_data.xlsx\",\n","             sheet_name='Label_Data')  "],"execution_count":null,"outputs":[]}]}